# Create dataset loaders, each corresponding to a different task for training
# This is usually the format required for continual learning, where the model is trained on different tasks sequentially.

trainset_loaders = []
testset_loaders = []

targets = [train_dataset[i][1] for i in range(len(train_dataset))]
labels = np.unique(targets)

num_concurrent_labels = num_classes // num_tasks


# label subsets (a form of class incremental learning). This means each task involves different classes from the CIFAR10 dataset.
for i in range(0, num_classes, num_concurrent_labels):
    concurrent_labels = labels[i: i + num_concurrent_labels]

    # Train subset
    concurrent_targets = np.isin(targets, concurrent_labels)
    filtered_indices = np.where(concurrent_targets)[0]
    train_ds_subset = torch.utils.data.Subset(train_dataset, filtered_indices)

    trainset_loader = torch.utils.data.DataLoader(train_ds_subset,
                                                  batch_size=batch_size,
                                                  shuffle=True,
                                                  num_workers=2)
    trainset_loaders.append(trainset_loader)

# Test subset
    test_targets = [test_dataset[i][1] for i in range(len(test_dataset))]
    concurrent_test_targets = np.isin(test_targets, concurrent_labels)
    filtered_test_indices = np.where(concurrent_test_targets)[0]
    test_ds_subset = torch.utils.data.Subset(test_dataset, filtered_test_indices)

    testset_loader = torch.utils.data.DataLoader(test_ds_subset,
                                                 batch_size=batch_size,
                                                 shuffle=False,
                                                 num_workers=2)
    testset_loaders.append(testset_loader)


# Create new train dataset loaders of random splits if hyperparameter is True
# This does not guarantee different classes for each task, as the split is random and not based on class labels.
if random_splits is True:
    trainset_loaders = []
    ds_length = len(train_dataset)
    splits_length = ds_length // num_tasks
    rand = np.random.permutation(len(train_dataset))
    ds_rand_parts = torch.utils.data.random_split(train_dataset,
                                                  [splits_length] * num_tasks,
                                                  generator=torch.Generator().manual_seed(0))
    for item in ds_rand_parts:
        trainset_loader = torch.utils.data.DataLoader(item,
                                                      batch_size=batch_size,
                                                      shuffle=True,
                                                      num_workers=2)
        trainset_loaders.append(trainset_loader)
